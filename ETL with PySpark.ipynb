{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, from_unixtime, monotonically_increasing_id, to_timestamp\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, IntegerType, TimestampType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Spark Session object \n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    # get filepath to song data file\n",
    "    # read song data file\n",
    "\n",
    "    song_data = input_data + \"song_data/*/*/*/*.json\"\n",
    "\n",
    "    song_data_schema = StructType([\n",
    "        StructField(\"artist_id\", StringType(), False),\n",
    "        StructField(\"artist_latitude\", StringType(), True),\n",
    "        StructField(\"artist_longitude\", StringType(), True),\n",
    "        StructField(\"artist_location\", StringType(), True),\n",
    "        StructField(\"artist_name\", StringType(), False),\n",
    "        StructField(\"song_id\", StringType(), False),\n",
    "        StructField(\"title\", StringType(), False),\n",
    "        StructField(\"duration\", DoubleType(), False),\n",
    "        StructField(\"year\", IntegerType(), False)\n",
    "    ])\n",
    "    \n",
    "    df = spark.read.json(song_data, schema=song_data_schema)\n",
    "    \n",
    "    # extract columns to create songs table\n",
    "    songs_table = df.select('song_id', 'title', 'artist_id', 'year', 'duration').distinct()\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    # songs_table\n",
    "    songs_table.write.partitionBy('year','artist_id').mode('overwrite').parquet(output_data + 'songs')\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df.select('artist_id', 'artist_name', 'artist_latitude', 'artist_longitude', 'artist_location').distinct()\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    # artists_table\n",
    "    artists_table.write.mode('overwrite').parquet(output_data + 'artists')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "    # get filepath to log data file\n",
    "    log_data = input_data + 'log_data/*/*/*.json'\n",
    "    \n",
    "    log_data_schema = StructType([\n",
    "        StructField(\"artist\", StringType(), True),\n",
    "        StructField(\"auth\", StringType(), True),\n",
    "        StructField(\"firstName\", StringType(), True),\n",
    "        StructField(\"gender\", StringType(), True),\n",
    "        StructField(\"itemInSession\", IntegerType(), True),\n",
    "        StructField(\"lastName\", StringType(), True),\n",
    "        StructField(\"length\", DoubleType(), True),\n",
    "        StructField(\"level\", StringType(), True),\n",
    "        StructField(\"location\", StringType(), True),\n",
    "        StructField(\"method\", StringType(), True),\n",
    "        StructField(\"page\", StringType(), True),\n",
    "        StructField(\"registration\", DoubleType(), True),\n",
    "        StructField(\"sessionId\", IntegerType(), True),\n",
    "        StructField(\"song\", StringType(), True),\n",
    "        StructField(\"status\", IntegerType(), True),\n",
    "        StructField(\"ts\", LongType(), True),\n",
    "        StructField(\"userAgent\", StringType(), True),\n",
    "        StructField(\"userId\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "    # read log data file\n",
    "    df = spark.read.json(log_data, schema=log_data_schema)\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df = df.filter(df['page'] == 'NextSong')\n",
    "\n",
    "    # extract columns for users table    \n",
    "    users_table = df.select(col('userId').alias('user_id'), col('firstName').alias('first_name'), col('lastName').alias('last_name'), 'gender', 'level').distinct()\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    users_table.write.mode('overwrite').parquet(output_data + 'users')\n",
    "    \n",
    "    # create timestamp column from original timestamp column\n",
    "    def get_seconds_since_unix(milliseconds):\n",
    "        return milliseconds / 1000\n",
    "    \n",
    "    get_seconds_since_unix = udf(get_seconds_since_unix, DoubleType())\n",
    "    \n",
    "    #Original dataset ts column has the milliseconds from unix information. We need to convert this value to seconds in order to pass it as parameter to a\n",
    "    # https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.functions.from_unixtime\n",
    "    df = df.withColumn('seconds_since_unix', get_seconds_since_unix(col('ts')))\n",
    "    df = df.withColumn('timestamp', from_unixtime(col('seconds_since_unix'), 'yyyy-MM-dd HH:mm:ss'))\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table = df.withColumn('start_time', col('timestamp')) \\\n",
    "        .withColumn('hour', hour('start_time')) \\\n",
    "        .withColumn('day', date_format('start_time', 'd')) \\\n",
    "        .withColumn('week', date_format('start_time', 'W')) \\\n",
    "        .withColumn('month', date_format('start_time', 'M')) \\\n",
    "        .withColumn('year', date_format(to_timestamp(col('start_time')), 'y')) \\\n",
    "        .withColumn('weekday', date_format('start_time', 'E')) \\\n",
    "        .select('timestamp', 'start_time','hour', 'day','week','month','year','weekday').distinct()\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.partitionBy('year', 'month').mode('overwrite').parquet(output_data + 'time')\n",
    "    \n",
    "    songs_table = spark.read.parquet(output_data + 'songs')\n",
    "    \n",
    "    songs_table.show()\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    songplays_table = df.join(songs_table, df.song == songs_table.title, how='inner') \\\n",
    "    .join(time_table.select('timestamp', 'start_time', col('year').alias('time_year'), col('month').alias('time_month')), \\\n",
    "          df.timestamp == time_table.timestamp) \\\n",
    "            .select('start_time', 'level', 'song_id', 'artist_id', 'location', \\\n",
    "                    col('userId').alias('user_id'), \\\n",
    "                    col('sessionId').alias('session_id') , \\\n",
    "                    col('userAgent').alias('user_agent'), \\\n",
    "                    col('time_year').alias('year'), \\\n",
    "                    col('time_month').alias('month'))\n",
    "    \n",
    "    songplays_table = songplays_table.withColumn('songplay_id', monotonically_increasing_id())\n",
    "    \n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.write.partitionBy('year', 'month').mode('overwrite').parquet(output_data + 'songplays')\n",
    "    \n",
    "    songplays_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = \"output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_song_data(spark, input_data, output_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+---------+----+------------------+\n",
      "|           song_id|               title| duration|year|         artist_id|\n",
      "+------------------+--------------------+---------+----+------------------+\n",
      "|SOAOIBZ12AB01815BE|I Hold Your Hand ...| 43.36281|2000|ARPBNLO1187FB3D52F|\n",
      "|SONYPOM12A8C13B2D7|I Think My Wife I...|186.48771|2005|ARDNS031187B9924F0|\n",
      "|SODREIN12A58A7F2E5|A Whiter Shade Of...|326.00771|   0|ARLTWXK1187FB5A3F8|\n",
      "|SOYMRWW12A6D4FAB14|The Moon And I (O...| 267.7024|   0|ARKFYS91187B98E58F|\n",
      "|SOWQTQZ12A58A7B63E|Streets On Fire (...|279.97995|   0|ARPFHN61187FB575F6|\n",
      "|SOUDSGM12AC9618304|Insatiable (Instr...|266.39628|   0|ARNTLGG11E2835DDB9|\n",
      "|SOPEGZN12AB0181B3D|Get Your Head Stu...| 45.66159|   0|AREDL271187FB40F44|\n",
      "|SOBBUGU12A8C13E95D|Setting Fire to S...|207.77751|2004|ARMAC4T1187FB3FA4C|\n",
      "|SOBAYLL12A8C138AF9|Sono andati? Fing...|511.16363|   0|ARDR4AC1187FB371A1|\n",
      "|SOBLGCN12AB0183212|James (Hold The L...|124.86485|1985|AR47JEX1187B995D81|\n",
      "|SOFFKZS12AB017F194|A Higher Place (A...|236.17261|1994|ARBEBBY1187B9B43DB|\n",
      "|SOBBXLX12A58A79DDA|Erica (2005 Digit...|138.63138|   0|AREDBBQ1187B98AFF5|\n",
      "|SOBKWDJ12A8C13B2F3|Wild Rose (Back 2...|230.71302|   0|AR36F9J1187FB406F1|\n",
      "|SONWXQJ12A8C134D94|The Ballad Of Sle...|  305.162|1994|ARNF6401187FB57032|\n",
      "|SOGNCJP12A58A80271|Do You Finally Ne...|342.56934|1972|ARB29H41187B98F0EF|\n",
      "|SOFNOQK12AB01840FC|Kutt Free (DJ Vol...|407.37914|   0|ARNNKDK1187B98BBD5|\n",
      "|SOWTBJW12AC468AC6E|Broken-Down Merry...|151.84934|   0|ARQGYP71187FB44566|\n",
      "|SOKEJEJ12A8C13E0D0|The Urgency (LP V...|245.21098|   0|ARC43071187B990240|\n",
      "|SOMVWWT12A58A7AE05|Knocked Out Of Th...|183.17016|   0|ARQ9BO41187FB5CF1F|\n",
      "|SOBONFF12A6D4F84D8|Tonight Will Be A...| 307.3824|1986|ARIK43K1187B9AE54C|\n",
      "+------------------+--------------------+---------+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+-----+-------+---------+--------+-------+----------+----------+----+-----+-----------+\n",
      "|start_time|level|song_id|artist_id|location|user_id|session_id|user_agent|year|month|songplay_id|\n",
      "+----------+-----+-------+---------+--------+-------+----------+----------+----+-----+-----------+\n",
      "+----------+-----+-------+---------+--------+-------+----------+----------+----+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_log_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
